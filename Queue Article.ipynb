{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for Engineers\n",
    "\n",
    "by Heinrich Hartmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Statistics plays a great role in modern IT operations.\n",
    "Monitoring systems collect a wealth of data from\n",
    "network-gear, operating system, application level metrics,\n",
    "that needs to be analyzed to derive vital information.\n",
    "For example, faults need to be detected early, we want\n",
    "to measure the performance the product or forecast\n",
    "the ressources that we will need to serve our users\n",
    "next month.\n",
    "\n",
    "Statistics is the art of extracting information from data.\n",
    "Hence, it becomes key to operate a modern IT system. \n",
    "Despite the community becoming more and more aware of this\n",
    "fact, ressources for learning the relevant statsitics for\n",
    "this domain are hard to find. \n",
    "\n",
    "In particular classical statistics appears not to be adequarte.\n",
    "Statistics courses at university depends on a great amount of\n",
    "pre-knowledge in probablity-, measure- and set-theory, which\n",
    "is very hard to digest. Moreover, it often focusses on \n",
    "parametric methods, like t-test that come with strong \n",
    "assumtptions on the distribution of the data ('normality')\n",
    "that are not met by operations data.\n",
    "\n",
    "The focus of many statistical treatments is largely outdated. The origins of statistics reach back to the 17-century, where computation was very expensive and data was a very sparse ressource. So mathematicians spent a lot of effort at avoiding calculations. The setting has has changed radically and allows different approaches to statistical problems.\n",
    "\n",
    "Have a look at this example form the book [H.O. Georgii - Stochastik, DeGruyter, 2002] used at my\n",
    "statistics class at university:\n",
    "\n",
    "> An furit-merchant gets a delivery of 10.000 oranges.\n",
    "> He want's to know how many of those are rotten.\n",
    "> To do so he takes a sample of 50 oranges, and counts the\n",
    "> numer of rotten ones x? Which deductions can he make\n",
    "> about the total number of rotten oranges?\n",
    "\n",
    "The chapter goes on with explaining various infrence methods.\n",
    "The example transated to our domain would go as follows:\n",
    "\n",
    "> A db admin want's to know how many requests took longer\n",
    "> than 1second to complete. He measures the duration of all\n",
    "> requests and count's the number of those which took longer\n",
    "> than 1second. Done.\n",
    "\n",
    "Wow that was easy. No statistics needed at all!\n",
    "\n",
    "Therefore, we will to take a different approach to Statistics\n",
    "in this article. Instead of presenting text book material, I'll\n",
    "present 5 episodes of relevant descriptive statistical methods\n",
    "that are accessible and relevant for case in point. I have\n",
    "tried to keep the mathematical prior knowledge to a minimum,\n",
    "by e.g. replacing formulas by source code whenever feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 1: Visualizing Data\n",
    "\n",
    "The most essential data analysis method is visualization.\n",
    "The human brain can process geomeric information much more rapidly than numbers or language. When presented with a suitable\n",
    "visualization we can capture relevant properties, like\n",
    "typical values and outliers almost instantly.\n",
    "\n",
    "In this episode we will run through the basic plotting\n",
    "methods and discuss their properties. For producing the\n",
    "plots, we have chosen to use the python toolchaing\n",
    "([ipython](http://ipython.org), [matplotlib](http://matplotlib.org), and [seaborn](http://stanford.edu/~mwaskom/software/seaborn/)).\n",
    "We will not show you, however, how to use this tools.\n",
    "There are a lot of alternative plotting tools (R, MATLAB) with\n",
    "accompaniating tutorials available online.\n",
    "Source code an Datasets can be found on <a href=\"http://github.com/HeinrichHartmann/Statistics-for-Engineers\">\n",
    "GitHub.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rugplots\n",
    "\n",
    "The most basic visualization method of a one-dimensional dataset \n",
    "`X = [x_1, ... ,x_n]`\n",
    "is the rugplot (cf. Figure 1). A rugplot consists of a single axis on which little lines, called 'rugs' are drawn for each sample.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_rugplot_3.png\">\n",
    "<figcaption>Figure 1: A Rugplot of web-requests rates</figcaption>\n",
    "</figure>\n",
    "\n",
    "Rugplots are suitable for all questions where the ordering of the samples is not relevant, like common values or outliers.\n",
    "Problems occure if there are multiple samples with the sampe\n",
    "value in the dataset. Those samples will be indistinguishable\n",
    "in the rugplot. This problem can be addressed by adding a\n",
    "small random displacement (jitter) to the samples.\n",
    "\n",
    "Despite it's simple and honest character, the rugplot is not very\n",
    "commonly used in practice, instead histograms or line plots are\n",
    "used, even if a rug-plot would have been more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Histograms are a popular visualization method for unordered\n",
    "one-dimensional data. Instead of drawing rugs on an axis,\n",
    "the we divide the axis into bins, and draw bars of a\n",
    "ceratin height on top of them, so that the number of\n",
    "samples within a bin is proportional to the area of the bar (cf. Figure 2).\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_histogram_2.png\">\n",
    "<figcaption>Figure 2: Histogram</figcaption>\n",
    "</figure>\n",
    "\n",
    "The use of a second dimension makes the histogram in\n",
    "many cases easier to comprehend than a rugplot. In\n",
    "particular, questions like: \"Which ratio of the samples\n",
    "lies below y?\" can be effecifly estimated by comparing\n",
    "areas. This convenience comes at the expenese of an\n",
    "extra dimension used and \n",
    "\n",
    "There is a lot more to tell about histograms, and our next\n",
    "episode is entirely devoted to this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot\n",
    "\n",
    "The scatterplot is the most basic visualization of\n",
    "a two-dimensional dataset. For each pair `x,y` of values\n",
    "we draw a point on a canvas, that has coordinates `(x,y)`\n",
    "in a cartesian coordinate system.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_scatterplot_2_1.png\" width=\"45%\" style=\"float:left\">\n",
    "<img src=\"img/example_scatterplot_2_2.png\" width=\"45%\">\n",
    "<figcaption>Figure 3: Scatterplots of request rates of two database nodes.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Scatterplot are a great tool to compare repeated measurments of\n",
    "two different quantities. In Figure 3 we have plotted the\n",
    "request rates of two different database nodes in a scatterplot.\n",
    "On the left side, the points are mainly concentrated on a diagonal line,\n",
    "which means that if one node servers a lot of requests then the other\n",
    "one is doing so as well. On the right side, the points are scattered\n",
    "all over the canvas, which represents a highly irregular load distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Plots\n",
    "\n",
    "Line plots are by far the most popular visualization method seen\n",
    "in practice. It is a special case of a scatter plot, where\n",
    "timestamps are plotted on the x-axis. In addition a line\n",
    "is drawn between consecutive points. Figure 4 shows an example\n",
    "of a line plot.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_lineplot.png\">\n",
    "<figcaption>Figure 4: A line plot of web-request rates.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "This addition provides the impression of a continues transition\n",
    "between the individual samples. This assumptions should always\n",
    "be challenged and taken with caution. E.g. Just because \n",
    "the CPU was idle at 1:00pm and 1:01pm, it does  not mean it did\n",
    "not do any work in between.\n",
    "\n",
    "Sometimes the actual datapoints are omitted altogether from the\n",
    "visualization and only the line is shown. This is a bad practice\n",
    "and should be avoided.\n",
    "\n",
    "Line plots are a great tool to surface time dependent patterns\n",
    "like periods or trends.When you are interested in time-independent\n",
    "question, like typical values, other methods like rug-plots might\n",
    "be more better suited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one to use?\n",
    "\n",
    "Chosse a suitable visualization for the question you want to answer.\n",
    "Is the time dependence important? Then line-plots are likely a good choice.\n",
    "If not, then rug-plots or histograms are likely a better tool.\n",
    "Do you want to compare different metrics to each other? Then consider using a scatter plot.\n",
    "\n",
    "Producing these plots should become a routine task for you.\n",
    "Your monitoring tool might be able to provide you with some\n",
    "of these already. \n",
    "To get the others, figure out how to export the relevant data\n",
    "and import them into the software tool of your choice (Python, R or Excel).\n",
    "Play arround with these visualizations and see how your machine data looks\n",
    "like.\n",
    "\n",
    "If you want to discover more visualization methods, check out the [seaborn gallery](http://stanford.edu/~mwaskom/software/seaborn/examples/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 2 -- Histograms\n",
    "\n",
    "Histograms occure in IT operations in two different\n",
    "roles: As visualization method and as aggregation method.\n",
    "\n",
    "To make sure we gain a complete understanding of histograms,\n",
    "we will start by building our very own histogram for the web-request\n",
    "reate data we have already met in Episode 1. \n",
    "Listing 1 contains a complete implementation, which we will discuss\n",
    "step by step in the next paragraphs.\n",
    "\n",
    "1. The first thing in building a histogram is to choose a range of values that should be covered. For this choice we need some prior knowledge about the dataset we are looking at. Minimum and maximum values are popular choices in  parctice. We choose $[500, 2200]$ as value range. \n",
    "2. Next we partition the value range into bins. Bins are often of equal size, but there is no need to follow this convention.  We represent the bin partition by a sequence of bin-boundaries (Line 4).\n",
    "3. We count how many samples of the given dataset are contained in each bin (Line 6-13). A value that lies on the bounday between two bins will be assigned to the higher bin.\n",
    "4. Finally, we have to produce a bar-chart, where each bar is based on one bin, and the bar-height is equal to sample count divided by bin width (Line 14-16). Figure 5 shows the resulting Histogram.\n",
    "\n",
    "<figure>\n",
    "<pre>\n",
    " 1| from matplotlib import pyplot as plt\n",
    " 2| import numpy as np\n",
    " 3| X = np.genfromtxt(\"DataSets/RequestRates.csv\", delimiter=\",\")[:,1]\n",
    " 4| bins = [500, 700, 800, 900, 1000, 1500, 1800, 2000, 2200]\n",
    " 5| bin_count = len(bins) - 1\n",
    " 6| sample_counts = [0] * bin_count\n",
    "10| for x in X:\n",
    "11|  for i in range(bin_count):\n",
    "12|       if (bins[i] <= x) and (x < bins[i + 1]):\n",
    "13|           sample_counts[i] += 1\n",
    "14| bin_widths = [ float(bins[i] - bins[i-1]) for i in range(1,  bin_count) ]\n",
    "15| bin_heights = [ count/width for count, width in zip(sample_counts, bin_widths) ]\n",
    "16| plt.bar(bins[:bin_count-1], width=bin_widths, height=bin_heights);\n",
    "</pre>\n",
    "<figcaption>Listing 1: Histogram</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/histogram_manual_1.png\">\n",
    "<figcaption>Figure 5: Result of a manual histogram implementation.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "The process of creating a histogram involves choosing the \n",
    "range and bin boundaries. Different choices can affect the\n",
    "visual appearance quite significantly. Figure 6 shows\n",
    "a histogram with 100 bins for the same data. Note that\n",
    "it closely resembles a rugplot. In the other extreme case,\n",
    "of a signle bin, the histogram degenerates to a single bar\n",
    "with height equal to the sample density.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/histogram_manual_2.png\">\n",
    "<figcaption>Figure 6: Histogram plot with value range (500, 2200) and 100 equally sized bins.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Software products make default choices for the value range and bin width. Typically the value range is taken to be the range of the data\n",
    "and equally spaced bins are used. There are several formulas for the\n",
    "number of bins which yield 'ideal' results under certain assumptions, \n",
    "in particular $\\sqrt{n}$ (Excel) and $3.5 \\sigma/n^{1/3}$ (Scott's rule) (cf. [Wikipedia, Histograms]).\n",
    "In practice, these choices do not yield satisfying results when applied\n",
    "to operations data like request latencies that contain many outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms as aggregation method\n",
    "\n",
    "When measuring high frequency data like, IO latencies, that\n",
    "can arive at a rates of more than 1.000samples/sec, it\n",
    "is no longer feasilble to store all individual samples. \n",
    "If we are willing to forget about ordering and sacrifice\n",
    "some accuracy we can save massive amounts of space,\n",
    "by using histogram datastructures.\n",
    "\n",
    "The essential idea is, instead of storing the indiviudal samples\n",
    "as a list, we can use the vector of bin counts that occures as\n",
    "an intermediate result in the histogram computation. E.g.\n",
    "in the above example in Listing 1, arrived at the following values:\n",
    "\n",
    "```python\n",
    "sample_count = [0, 10, 8, 4, 25, 23, 4, 2]\n",
    "```\n",
    "\n",
    "The precise memory representation used for storing histograms does vary.\n",
    "The important point is that we have the sample count of each bin available.\n",
    "\n",
    "Histograms allow approximate computation of various summary \n",
    "statistics, like mean values and quantiles that we will meet,\n",
    "in the following episodes. The acceived percision is dependent\n",
    "on the bin sizes.\n",
    "\n",
    "Also histograms can be aggregated easily. If you have\n",
    "request latencies available for each node of a database cluster\n",
    "in histograms with the same bin choices, then you can derive \n",
    "the latency distribution of the whole cluster by adding the \n",
    "sample counts for each bin. We can use the aggregated histogram\n",
    "to calculate mean values and quantiles over the whole cluster.\n",
    "\n",
    "This is in contrast to the situation when we computed mean values\n",
    "or quantiles for the nodes individually. It is not possible to\n",
    "derive, e.g. the 99%-percentile of the whole cluster from\n",
    "the 99%-percentiles of the individual nodes\n",
    "(cf. [Theo Schlossnagle - Problems with Math])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDR Histograms\n",
    "\n",
    "HDR histograms provide a pragmatic choice for bin-width that allows\n",
    "a memory efficient representations suitable to captureing data on\n",
    "a very wide range that is common to machine generated data \n",
    "like, e.g. IO latencies. At the same time HDR histograms tend\n",
    "to produce acceptable visual representations in practice.\n",
    "\n",
    "The basic idea is to choose the bin width adaptive to the current\n",
    "order of magnitued. E.g. between 1 and 10 the bin size is 0.1,\n",
    "with bin boundaries: $1,1.1,1.2,\\dots,9.9,10$. Similaraly between\n",
    "between 10 and 100 the bin size is 1, with boundaries $10,11,12,\\dots,100$.\n",
    "This pattern is repeasted for all powers of ten, so that we \n",
    "have 90 bins between $10^k$ and $10^{k+1}$. Typical range of $k$\n",
    "are $-127\\dots128$, often an extra bin for the $0$-value is appended.\n",
    "\n",
    "\n",
    "The general definition is a little bit more complex and lengthy,\n",
    "so that we don't give it here. The interested reader is referred \n",
    "to [http://hdrhistogram.org/] for more details and a memory-efficient\n",
    "implementation.\n",
    "\n",
    "From the above description it should be apparent, that\n",
    "HDR histograms span an extremly large value range ($[10^{-127}, 10^{128})$).\n",
    "The bin-sizes are very similiar to the float number precisions: \n",
    "The larger the value, the less precision is available. In addition\n",
    "the bin boundaries are independent on the dataset. Hence the aggregation\n",
    "technique described in the last paragraph applies to HDR histograms.\n",
    "\n",
    "### Histograms as Heatmaps\n",
    "\n",
    "If we are interested in observing the change of data distributions over time,\n",
    "we have to add an additional dimension to the histogram plot. \n",
    "A convenient method to do so, is represent the sample densities as a heatmap\n",
    "instead of a bar chart. Figure zz shows the request rate data visualized\n",
    "in such a way. Light colors mean low sample density, dark colors signal\n",
    "high sample desnity.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/histogram_heatmap_2.png\" />\n",
    "<figcaption>Figure zz: Request rate histogram (50 bins) preseted as heatmap.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can combine multiple such histogram heatmaps that were captured over time\n",
    "to a single two-dimensional heatmap. \n",
    "\n",
    "\n",
    "**War story:** \n",
    "Figure #heatmap shows a particularly interesting example of such a visualization for a sequence\n",
    "of (HDR) histograms of web request latencies. Note that the distribution of the data is bi-modal\n",
    "with one mode constant around `~5 ms` and another more diffuse mode ascending from `~10ms` to `~50ms`.\n",
    "In this particular case the second mode was caused by a bug in a session handler, that caused\n",
    "the addition of new entries to an in memory list. The particular list had to be traversed for\n",
    "each incoming request causing extended delays. Looking carefully, even the logarithmic growth\n",
    "of the average traversal time can be spotted.\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"img/Circonus_Histogram.png\"/></center>\n",
    "<figcaption>Figure #heatmap: Request latency heatmap over time in Circonus.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 3 -- Classical Summary Statistics\n",
    "\n",
    "The aim of summary statistics is to provide a summary of\n",
    "the essential features of a dataset.\n",
    "It is the numeric equivalent of an elevator pitch in a business context. If you just want the essential information, you\n",
    "should not need to look at all the details.\n",
    "\n",
    "A good summary statistic should be able to answer questions\n",
    "like \"What are typical values?\" or \"How much variation is in\n",
    "the data?\". A desireable property is robustnes against outliers.\n",
    "A single faulty measurement should not change a rough description\n",
    "of the dataset.\n",
    "\n",
    "In this episode we will discuss the classical summary statistics:\n",
    "mean values and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Value\n",
    "\n",
    "The _mean value_ or _average_ of a dataset $X=[x_1, \\dots, x_n]$ is defined as\n",
    "\n",
    "$$ mean(x_1, \\dots, x_n) = \\frac{1}{n} \\sum_{i=1}^n x_i. $$\n",
    "\n",
    "or when expressed as python code:\n",
    "\n",
    "```python\n",
    "def mean(X): return sum(X) / len(X)\n",
    "```\n",
    "\n",
    "The mean values has the physical interpretation of the center of\n",
    "mass if we place weights of equal weight on the points $x_i$\n",
    "on a (massless) axes. When the values of $x_i$ are close together,\n",
    "the mean value is a good representative of a typical sample.\n",
    "Contrary, when the samples are concentrated at several centers,\n",
    "or outliers are present, the mean value can be far from each\n",
    "individul datapoint (cf. Figure yy).\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/rugplot_mean.png\">\n",
    "<figcaption>Figure yy: Rugplot of a two-modal dataset (blue) with mean value (red).</figcaption>\n",
    "</figure>\n",
    "\n",
    "Mean values are abundant in IT operations.\n",
    "One common application of mean values is data rollup.\n",
    "When multiple samples arrived during a sampling period of e.g. one minute, the mean value is calculated as a  \"one-minute rollup\" and\n",
    "stored instead of the original samples. Similarly, if we have\n",
    "data available for every minute, but are only interested in\n",
    "hour intervals, we can \"rollup the data by the hour\", by taking mean values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Erosion\n",
    "\n",
    "When viewing metrics as line-plots in a monitoring system a phenomenon we\n",
    "call 'spike erosion' can often be observed.\n",
    "\n",
    "To reproduce this phenomeon, pick a metric (e.g. ping latencies) that experiences spikes at\n",
    "discrete points in time, and zoom in on one of those spikes and read off the height off\n",
    "the spike at the y-axis. Now zoom out of the graph to a range of\n",
    "one month, and read off the height of the same spike again.\n",
    "Are they equal? \n",
    "\n",
    "Figure #spikeerosion shows an example for a graph in the Circonus monitoring system. The spike height has decreased from `0.8` to `0.3`!\n",
    "\n",
    "How is that possible? The result is an artifact of a rollup procedure\n",
    "that is commonly used when displaying graphs over long time ranges.\n",
    "The amount of data gathered over the period of one months (i.e. `>40K` minutes) is larger than the amount of pixels available for the plot.\n",
    "Therefore the data has to be rolled-up to larger time periods before\n",
    "it can be plotted. When the mean value is used for the rollups,\n",
    "the single spike is averaged with an increasing number of 'normal' samples and hence decreases in height.\n",
    "\n",
    "How to do better? The immediate way to address this problem is to choose an alternative rollup method, like max-values. However, \n",
    "one sacrifices information about typical values in this way.\n",
    "Another, more elegant, solution is to rollup values as histograms,\n",
    "and display a 2d heatmap instead of a line plot for larger view\n",
    "ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deviation Measures\n",
    "\n",
    "1. The _maximal deviation_ is defined as\n",
    "\n",
    "$$ maxdev(x_1,\\dots,x_n) = max \\{ |x_i - \\mu| \\,|\\, i=1,\\dots,n\\}.$$\n",
    "\n",
    "2. The _mean absolute deviation_ is defined as\n",
    "\n",
    "$$ mad(x_1,\\dots,x_n) = \\frac{1}{n} \\sum_{i=1}^n |x_i - \\mu|.$$\n",
    "\n",
    "3. The _standard deviation_ is defined as\n",
    "\n",
    "$$ \\sigma = stddev(x_1,\\dots,x_n) =  \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2}.$$\n",
    "\n",
    "\n",
    "* Measure the 'typical' displacement from the mean value.\n",
    "* Standard deviation is popular because it has extremely nice mathematical properties.\n",
    "* Part of a continues family of deviation measures (p-norms).\n",
    "\n",
    "Remarks:\n",
    "- Standard deviations are very sensitive to ourliers\n",
    "- Mean deviation is better but still linear impact on outliers.\n",
    "\n",
    "\n",
    "IMAGE\n",
    "\n",
    "## Caution with Standard Deviation\n",
    "\n",
    "- Everybody Learns about standard deviation in school\n",
    "- Beautiful mathematical properties!\n",
    "- Everybody knows \n",
    "  - \"68% of data falls within 1 std-dev of the mean\"\n",
    "  - \"95% falls within 2 std-dev of the mean\"\n",
    "  - \"99.7\" falls within 3 std-dev of the mean\"\n",
    "* \"Problem is: this is utter nonsense\". Only true for normally distributed data.\n",
    "\n",
    "* Not good for measuring outliers!\n",
    "\n",
    "\n",
    "_Source:_ Janert - Data Analysis with Open Source Tools\n",
    "\n",
    "## War Story:\n",
    "\n",
    "- Looking at SLA for DB response times\n",
    "- Outlier defined as value larger than $\\mu + 3\\sigma$\n",
    "- Look at code: Takes '0.3' percentile!\n",
    "- So always have outliers.\n",
    "- And 0.3-percentile was way too large (hours of latency).\n",
    "- Programmer changed code for 1%, 5%, 10% quantiles.\n",
    "- Finally handcoded a threshold\n",
    "- The SLA was never changed\n",
    "\n",
    "Source: Janert - Data Analysis with Open Source Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 4 -- Quantiles and Outliers\n",
    "\n",
    "The \n",
    "\n",
    "* Classical summary statistics good for describing the body of the distribution\n",
    "* Need information about the tail of the distributions, e.g. for writing good SLAs\n",
    "* Determine outliers in a dataset\n",
    "\n",
    "### Comulative Distribution Functions\n",
    "\n",
    "The (empirical) cumulative distribution function of a dataset $X$, is defined as:\n",
    "\n",
    "$$ CDF(y) = \\# \\{ i \\, | \\, x_i \\leq y \\} / \\# X $$\n",
    "\n",
    "So CDF(y) = the ratio of samples that are lower than $y$.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Properties:\n",
    "\n",
    "* $0 \\leq CDF(y) \\leq 1$\n",
    "* CDF is monotonically increasing\n",
    "\n",
    "## Quantiles and Percentiles\n",
    "\n",
    "* Complement or Inverse to CDFs:\n",
    "  - CDF: The ratio of samples was below 100 was `<CDF>`\n",
    "  - Quantile: 90% of all queries where faster than `<quantile>`\n",
    "* Qunatile parameter is indpendent of value range\n",
    "\n",
    "Examples:\n",
    "\n",
    "* The minimum is a 0-quantile\n",
    "* A median is a 0.5-quantile\n",
    "* The maximum is a 1-quantile\n",
    "\n",
    "Special names:\n",
    "\n",
    "* Quartiles: $k/4$-quantiles\n",
    "* Percentiles: $k/100$-quantiles\n",
    "\n",
    "### Median\n",
    "\n",
    "A _median value_ for $x_1, \\dots, x_n$ is number $m$ such that\n",
    "  \n",
    " $$ \\# \\{ i \\,|\\, x_i \\leq m \\} = \\# \\{ i \\,|\\, x_i \\geq m \\}. $$\n",
    "\n",
    "So the number of samples smaller than $m$ is equal to the number of samples larger than $m$.\n",
    "(Both should be roughly $n/2$).\n",
    "  \n",
    "Remark:\n",
    "\n",
    "* A Median always exists\n",
    "* Median is not unique\n",
    "* Can be computed in linear time\n",
    "* Not influenced by outliers (robust)\n",
    "\n",
    "\n",
    "### General Definition of Quantiles\n",
    "Let $0\\leq q \\leq 1$ be a real number. A $q$-quantile for $X$ is a value $y$ such that, if\n",
    "\n",
    "$$ \\#\\{i \\,|\\, x_i \\leq y \\} \\geq q \\cdot n  $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\#\\{i \\,|\\, x_i \\geq y \\} \\geq (1-q) \\cdot n $$\n",
    "\n",
    "Roughly speaking, $y$ divides $X$ in $q \\cdot n$ samples that are lower than $y$ and $(1-q) \\cdot n$ samples that are larger than $y$.\n",
    "\n",
    "Remarks:\n",
    "\n",
    "* Quantiles always exists\n",
    "* Non unique (like median)\n",
    "* Lot's of ways to choose a quantile function, i.e. interpolate between $s_a$ and $s_b$ cf.  \n",
    "  http://en.wikipedia.org/wiki/Quantile#Estimating_the_quantiles_of_a_population\n",
    "  \n",
    "### Using quantiles and CDF to monitor/formulate SLAs\n",
    "\n",
    "CDFs can be used to determine service levels.\n",
    "\n",
    "1. Measure response time latencies each minute over 1h\n",
    "2. Calculate `CDF(<max tolerable latency>)` for each 1h window\n",
    "3. Good service when `CDF = 1`\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_cdf_sla.png\">\n",
    "<figcaption>CDF(0.05) over 1h windows for Twitter Ping Latencies</figcaption>\n",
    "</figure>\n",
    "\n",
    "* Histograms are a much better representation of actual API usage\n",
    "* Calculate CDF over all requestst that arrived in 1h\n",
    "* Catch delays that affect only a small fraction of the requests\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_histogram_IVP.png\">\n",
    "<figcaption>Histogram metric with CFD(3) over 1h windows</figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IQR and Outliers\n",
    "\n",
    "The interquartile range of a sample X is defined as:\n",
    "\n",
    "`IQR(X) = Q(0.75,X) - Q(0.25,X)`\n",
    "\n",
    "It is a robust measure for variance of the data. Good alternative to standard / mean deviation.\n",
    "\n",
    "**Def.** (Tukey, 1969) a k-outlier is a data point X which is either\n",
    "\n",
    "* larger than `Q(0.75) + k * IQR(X)` or\n",
    "* smaller than `Q(0.25) - k * IQR(X)`.\n",
    "\n",
    "An outlier (without k) is an 1.5-outlier.\n",
    "\n",
    "# Tukey's Boxplots\n",
    "\n",
    "Show:\n",
    "\n",
    "* Median\n",
    "* Box around 0.25 and 0.75 Quantiles\n",
    "* \"whiskers\" from min to max\n",
    "* points for outliers\n",
    "\n",
    "Allows visual clues:\n",
    "\n",
    "* Where is the data concentracted?\n",
    "* How far is it spread?\n",
    "* How skew is the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 4b -- Comparing Distributions\n",
    "\n",
    "- Problem: Code change. Want to compare performance. Did it change?\n",
    "  -> For the better?\n",
    "  -> For the worse?\n",
    "- Classical T-Test has normal assumption in it\n",
    "- Use: Kolmogorov-Smirnov distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 5 -- Forecasting\n",
    "\n",
    "- Main use case forecast data for capacity planning\n",
    "- Regression can be used to capture linear and exponential trends\n",
    "- Holt-Winters method can be used to forecast periodic data \n",
    "  but many parameters, lot's of things can go wrong!\n",
    "  Need error estimates for this.\n",
    "  \n",
    "- Avoid forecasting complex behavior (if you can)\n",
    "- Use regression or robust regressions for forecasting\n",
    "- How well did we do? Evaluate using goodness of fit measure, R2.\n",
    "\n",
    "\n",
    "## Regression Method\n",
    "\n",
    "* Given two vectors `X,Y` of the same length\n",
    "* Q: Can we predict `Y[i]` from knowing `X[i]`\n",
    "* Can we find a (mathematical) function $f$ so that $f(x)$ is close to $y$?\n",
    "\n",
    "* Parametric Ansatz for $f(x) = f(\\theta; x)$\n",
    "* Define residuals $e_i$ by:\n",
    "\n",
    "$$x_i = f(\\theta; t_i) + e_i$$\n",
    "\n",
    "* Loss function:\n",
    "\n",
    "$$ Loss(\\theta) = \\sum_i e_i^2 = \\sum_i (y_i - f(\\theta;t_i))^2 $$ \n",
    "\n",
    "* Chosse $\\hat{\\theta}$ by minimizing $Loss(\\theta)$.\n",
    "\n",
    "* A _goodness of fit_ measure is the minimal loss $MinLoss=Loss(\\hat{\\theta})$\n",
    "\n",
    "## Warmup: Constant Model\n",
    "\n",
    "* Parametric Ansatz for f: $f(x) = a$, constant.\n",
    "* Quadratic loss function:\n",
    "\n",
    "$$ Loss(a) = \\sum_i (y_i - a)^2 $$\n",
    "\n",
    "* Minimize loss function (_using calculus!_) gives __mean value__:\n",
    "\n",
    "$$ \\hat{a} = \\frac{1}{n} \\sum_i y_i = \\mu_Y $$\n",
    "\n",
    "$$ MinLoss(X,Y) = \\sum_i (y_i - \\mu_Y)^2$$\n",
    "\n",
    "* This quantity is also known as ($n$ times) the __variance__. So:\n",
    "* $Var(Y)$ measures goodness of constant fit!\n",
    "\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "* Parametric Ansatz for f:\n",
    "  $$f(a,b; t) = bt + a, \\quad \\theta=(a,b)$$\n",
    "\n",
    "* Quadratic loss function:\n",
    "\n",
    "$$ Loss(a,b) = \\sum_i e_i^2 = \\sum_i (y_i - f(t_i))^2 =  \\sum_i (y_i - b t_i - a)^2 $$ \n",
    "\n",
    "$$ = a^2 A + ab \\cdot B + b^2 \\cdot C + D$$\n",
    "\n",
    "for some variables $A,B,C,D$ depending on $X,Y$.\n",
    "\n",
    "* Regression: Minimize $Loss(a,b)$ with respect to parameters $(a,b)$.\n",
    "\n",
    "* Since $Loss(a,b)$ is quadratic, it has a unique minimum which is easy to compute:\n",
    "\n",
    "$$\\hat{b} =  \\frac{\\sum_i (x_i - \\mu_x)(y_i - \\mu_y)}{\\sum_i(x_i - \\mu_x)^2} = Cov(X,Y) / Var(X) = \\rho_{X,Y} \\frac{\\sigma_Y}{\\sigma_X}$$\n",
    "\n",
    "$$\\hat{a} = \\mu_Y - \\hat{b} \\mu_X $$\n",
    "\n",
    "Remarks:\n",
    "\n",
    "* See (http://en.wikipedia.org/wiki/Simple_linear_regression) for a derivation.\n",
    "\n",
    "* Works also for more complex functions (e.g. polynomials)\n",
    "\n",
    "* Name \"linear\" regression comes from \"quadratic\" (!) loss function.\n",
    "\n",
    "* Note that Pearson Correlation appears in formula for $\\hat{b}$.\n",
    "\n",
    "\n",
    "\n",
    "## Exponential regression\n",
    "\n",
    "Use a different Model\n",
    "\n",
    "$$f(t) = exp(a \\cdot t + b)$$\n",
    "\n",
    "Trick: Use log to reduce to linear case.\n",
    "\n",
    "* Forecast exponential growth\n",
    "* Compute Compound Annual Growth rate\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_exp_forecast.png\">\n",
    "<figcaption>Exponential user-statistics forecast.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "## Goodness of fit\n",
    "\n",
    "* Natural measure for goodness of fit is the Loss of the ideal fit viewed as a function of $X$ and $Y$:\n",
    "\n",
    "$$ MinLoss(X,Y) = \\sum_i (y_i - \\hat{b} x_i - \\hat{a})^2 $$\n",
    "\n",
    "* Problem: Scales (quadratic) with $Y$: $MinLoss(X,3 Y)=9 MinLoss(Y)$ without the regression getting better or worse. \n",
    "\n",
    "* Use loss of constant regression (ie. mean value):\n",
    "\n",
    "$$ ConstLoss(Y) = \\sum_i (y_i - \\mu_y)^2  = n \\cdot Var(Y) $$\n",
    "  \n",
    "**Def:** The $R^2-Value$ value is\n",
    "\n",
    "$$ R^2 = 1 - \\frac{MinLoss(X,Y)}{ConstLoss(Y)}$$\n",
    "\n",
    "* Lies in [0,1].\n",
    "* Perfect fit if $R^2=1$\n",
    "\n",
    "* Scaling invariance: Does not change when $X,Y$ is replaced by $a X, b Y$.\n",
    "\n",
    "* Can be viewed as variance ratio $SS_{reg} / SS_{tot}$.\n",
    "  http://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "\n",
    "**Proposition:**\n",
    "\n",
    "$$ R^2 = \\frac{Cov(X,Y)^2}{Var(X) \\cdot Var(Y)} = (\\frac{Cov(X,Y)}{\\sigma_X \\cdot \\sigma_Y})^2$$\n",
    "\n",
    "The (Pearson) _correlation_ $\\rho = \\sqrt{R^2}$ is defined as measures how well a linear model fits the plot.\n",
    "\n",
    "By Cauchy-Schwary inequality we have $0\\leq \\rho^2 \\leq 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have covered most important applications of statistics in IT Operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## About the Author\n",
    "\n",
    "Heinrich Hartmann is Chief Data Scientist for Circonus. He earned his\n",
    "PhD in pure Mathematics from the University of Bonn (Germany) on\n",
    "geometric aspects of string theory and worked as a researcher for the\n",
    "University of Oxford (UK) afterwards.  In 2012 he transitioned into\n",
    "computer science and worked as independent consultant for a number of\n",
    "different companies and research institutions. He is now leading the\n",
    "development of data analytics for the Circonus monitoring product.\n",
    "\n",
    "## About Circonus\n",
    "\n",
    "Circonus provides analytics and monitoring for Web-Scale IT. Developed\n",
    "specifically for the requirements of DevOps, the Circonus platform\n",
    "delivers alerts, graphs, dashboards and machine-learning intelligence\n",
    "that help to optimize not just your operations, but also your\n",
    "business. Proprietary Database technology and Analytics tools enable\n",
    "Circonus to provide forensic, predictive, and automated analytics\n",
    "capabilities that no other product can match, and at a scale that\n",
    "other products can only dream of.\n",
    "\n",
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
