{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for Engineers\n",
    "\n",
    "by Heinrich Hartmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Statistics plays a great role in modern IT operations.\n",
    "Monitoring systems collect a wealth of data from\n",
    "network-gear, operating systems, applications and other components,\n",
    "that needs to be analyzed to derive vital information.\n",
    "For example, faults need to be detected early, application\n",
    "performance needs to be measured, and the capacity needed to\n",
    "serve the load of next month needs to be forecasted.\n",
    "\n",
    "Statistics is the art of extracting information from data.\n",
    "Hence, it becomes key to operate a modern IT system. \n",
    "Despite the community becoming more and more aware of this\n",
    "fact, ressources for learning the relevant statsitics for\n",
    "this domain are hard to find.\n",
    "\n",
    "In particular classical statistics appears not to be adequarte.\n",
    "Statistics courses at university depends on a great amount of\n",
    "pre-knowledge in probablity-, measure- and set-theory, which\n",
    "is very hard to digest. Moreover, it often focusses on \n",
    "parametric methods, like t-test that come with strong \n",
    "assumtptions on the distribution of the data ('normality')\n",
    "that are not met by operations data.\n",
    "\n",
    "This lack of relevance can be explained by history.\n",
    "The origins of statistics reach back to the 17-century, where computation was very expensive and data was a very sparse ressource.\n",
    "So mathematicians spent a lot of effort at avoiding calculations. \n",
    "The setting has has changed radically and allows different approaches to statistical problems.\n",
    "\n",
    "Have a look at this example form the book [H.O. Georgii - Stochastik, DeGruyter, 2002] used at my\n",
    "statistics class at university:\n",
    "\n",
    "> An furit-merchant gets a delivery of 10.000 oranges.\n",
    "> He want's to know how many of those are rotten.\n",
    "> To do so he takes a sample of 50 oranges, and counts the\n",
    "> numer of rotten ones x? Which deductions can he make\n",
    "> about the total number of rotten oranges?\n",
    "\n",
    "The chapter goes on with explaining various infrence methods.\n",
    "The example transated to our domain would go as follows:\n",
    "\n",
    "> A db admin want's to know how many requests took longer\n",
    "> than one second to complete. He measures the duration of all\n",
    "> requests and count's the number of those which took longer\n",
    "> than one second. Done.\n",
    "\n",
    "The abundance of computing ressources has completely eliminated\n",
    "the need for elaborate estimations.\n",
    "\n",
    "Therefore, we will to take a different approach to Statistics\n",
    "in this article. Instead of presenting text book material, I'll\n",
    "present a few episodes of relevant descriptive statistical methods\n",
    "that are accessible and relevant for case in point. I have\n",
    "tried to keep the mathematical prior knowledge to a minimum,\n",
    "by e.g. replacing formulas by source code whenever feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 1: Visualizing Data\n",
    "\n",
    "The most essential data analysis method is visualization.\n",
    "The human brain can process geomeric information much more\n",
    "rapidly than numbers or language. When presented with a suitable\n",
    "visualization we can capture relevant properties, like\n",
    "typical values and outliers almost instantly.\n",
    "\n",
    "In this episode we will run through the basic plotting\n",
    "methods and discuss their properties. For producing the\n",
    "plots, we have chosen to use the python toolchaing\n",
    "([ipython](http://ipython.org), [matplotlib](http://matplotlib.org), and [seaborn](http://stanford.edu/~mwaskom/software/seaborn/)).\n",
    "We will not show you, however, how to use this tools.\n",
    "There are a lot of alternative plotting tools (R, MATLAB) with\n",
    "accompaniating tutorials available online.\n",
    "Source code an Datasets can be found on <a href=\"http://github.com/HeinrichHartmann/Statistics-for-Engineers\">\n",
    "GitHub.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rugplots\n",
    "\n",
    "The most basic visualization method of a one-dimensional dataset \n",
    "`X = [x_1, ... ,x_n]`\n",
    "is the rugplot (cf. Figure 1). A rugplot consists of a single axis on which little lines, called 'rugs' are drawn for each sample.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_rugplot_3.png\">\n",
    "<figcaption>Figure 1: A Rugplot of web-requests rates</figcaption>\n",
    "</figure>\n",
    "\n",
    "Rugplots are suitable for all questions where the ordering of the samples is not relevant, like common values or outliers.\n",
    "Problems occure if there are multiple samples with the sampe\n",
    "value in the dataset. Those samples will be indistinguishable\n",
    "in the rugplot. This problem can be addressed by adding a\n",
    "small random displacement (jitter) to the samples.\n",
    "\n",
    "Despite it's simple and honest character, the rugplot is not very\n",
    "commonly used in practice, instead histograms or line plots are\n",
    "used, even if a rug-plot would have been more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Histograms are a popular visualization method for unordered\n",
    "one-dimensional data. Instead of drawing rugs on an axis,\n",
    "the we divide the axis into bins, and draw bars of a\n",
    "ceratin height on top of them, so that the number of\n",
    "samples within a bin is proportional to the area of the bar (cf. Figure 2).\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_histogram_2.png\">\n",
    "<figcaption>Figure 2: Histogram</figcaption>\n",
    "</figure>\n",
    "\n",
    "The use of a second dimension makes the histogram in\n",
    "many cases easier to comprehend than a rugplot. In\n",
    "particular, questions like: \"Which ratio of the samples\n",
    "lies below y?\" can be effecifly estimated by comparing\n",
    "areas. This convenience comes at the expenese of an\n",
    "extra dimension used and additional choices that\n",
    "have to be made about value ranges and bin sizes.\n",
    "\n",
    "There is a lot more to tell about histograms, and our next\n",
    "episode is entirely devoted to this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot\n",
    "\n",
    "The scatterplot is the most basic visualization of\n",
    "a two-dimensional dataset. For each pair `x,y` of values\n",
    "we draw a point on a canvas, that has coordinates `(x,y)`\n",
    "in a cartesian coordinate system.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_scatterplot_2_1.png\" width=\"45%\" style=\"float:left\">\n",
    "<img src=\"img/example_scatterplot_2_2.png\" width=\"45%\">\n",
    "<figcaption>Figure 3: Scatterplots of request rates of two database nodes in samples per second.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Scatterplot are a great tool to compare repeated measurments of\n",
    "two different quantities. In Figure 3 we have plotted the\n",
    "request rates of two different database nodes in a scatterplot.\n",
    "On the left side, the points are mainly concentrated on a diagonal line,\n",
    "which means that if one node servers a lot of requests then the other\n",
    "one is doing so as well. On the right side, the points are scattered\n",
    "all over the canvas, which represents a highly irregular load distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Plots\n",
    "\n",
    "Line plots are by far the most popular visualization method seen\n",
    "in practice. It is a special case of a scatter plot, where\n",
    "timestamps are plotted on the x-axis. In addition a line\n",
    "is drawn between consecutive points. Figure 4 shows an example\n",
    "of a line plot.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_lineplot.png\">\n",
    "<figcaption>Figure 4: A line plot of web-request rates.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "This addition provides the impression of a continues transition\n",
    "between the individual samples. This assumptions should always\n",
    "be challenged and taken with caution. E.g. Just because \n",
    "the CPU was idle at 1:00pm and 1:01pm, it does  not mean it did\n",
    "not do any work in between.\n",
    "\n",
    "Sometimes the actual datapoints are omitted altogether from the\n",
    "visualization and only the line is shown. This is a bad practice\n",
    "and should be avoided.\n",
    "\n",
    "Line plots are a great tool to surface time dependent patterns\n",
    "like periods or trends.When you are interested in time-independent\n",
    "question, like typical values, other methods like rug-plots might\n",
    "be more better suited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one to use?\n",
    "\n",
    "Chosse a suitable visualization for the question you want to answer.\n",
    "Is the time dependence important? Then line-plots are likely a good choice.\n",
    "If not, then rug-plots or histograms are likely a better tool.\n",
    "Do you want to compare different metrics to each other? Then consider using a scatter plot.\n",
    "\n",
    "Producing these plots should become a routine task for you.\n",
    "Your monitoring tool might be able to provide you with some\n",
    "of these already. \n",
    "To get the others, figure out how to export the relevant data\n",
    "and import them into the software tool of your choice (Python, R or Excel).\n",
    "Play arround with these visualizations and see how your machine data looks\n",
    "like.\n",
    "\n",
    "If you want to discover more visualization methods, check out the [seaborn gallery](http://stanford.edu/~mwaskom/software/seaborn/examples/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 2 -- Histograms\n",
    "\n",
    "Histograms occure in IT operations in two different\n",
    "roles: As visualization method and as aggregation method.\n",
    "\n",
    "To make sure we gain a complete understanding of histograms,\n",
    "we will start by building our very own histogram for the web-request\n",
    "reate data we have already met in Episode 1. \n",
    "Listing 1 contains a complete implementation, which we will discuss\n",
    "step by step in the next paragraphs.\n",
    "\n",
    "1. The first thing in building a histogram is to choose a range of values that should be covered. For this choice we need some prior knowledge about the dataset we are looking at. Minimum and maximum values are popular choices in  parctice. We choose $[500, 2200]$ as value range. \n",
    "2. Next we partition the value range into bins. Bins are often of equal size, but there is no need to follow this convention.  We represent the bin partition by a sequence of bin-boundaries (Line 4).\n",
    "3. We count how many samples of the given dataset are contained in each bin (Line 6-13). A value that lies on the bounday between two bins will be assigned to the higher bin.\n",
    "4. Finally, we have to produce a bar-chart, where each bar is based on one bin, and the bar-height is equal to sample count divided by bin width (Line 14-16). Figure 5 shows the resulting Histogram.\n",
    "\n",
    "<figure>\n",
    "<pre>\n",
    " 1| from matplotlib import pyplot as plt\n",
    " 2| import numpy as np\n",
    " 3| X = np.genfromtxt(\"DataSets/RequestRates.csv\", delimiter=\",\")[:,1]\n",
    " 4| bins = [500, 700, 800, 900, 1000, 1500, 1800, 2000, 2200]\n",
    " 5| bin_count = len(bins) - 1\n",
    " 6| sample_counts = [0] * bin_count\n",
    "10| for x in X:\n",
    "11|  for i in range(bin_count):\n",
    "12|       if (bins[i] <= x) and (x < bins[i + 1]):\n",
    "13|           sample_counts[i] += 1\n",
    "14| bin_widths = [ float(bins[i] - bins[i-1]) for i in range(1,  bin_count) ]\n",
    "15| bin_heights = [ count/width for count, width in zip(sample_counts, bin_widths) ]\n",
    "16| plt.bar(bins[:bin_count-1], width=bin_widths, height=bin_heights);\n",
    "</pre>\n",
    "<figcaption>Listing 1: Histogram</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/histogram_manual_1.png\">\n",
    "<figcaption>Figure 5: Result of a manual histogram implementation.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "The process of creating a histogram involves choosing the \n",
    "range and bin boundaries. Different choices can affect the\n",
    "visual appearance quite significantly. Figure 6 shows\n",
    "a histogram with 100 bins for the same data. Note that\n",
    "it closely resembles a rugplot. In the other extreme case,\n",
    "of a signle bin, the histogram degenerates to a single bar\n",
    "with height equal to the sample density.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/histogram_manual_2.png\">\n",
    "<figcaption>Figure 6: Histogram plot with value range (500, 2200) and 100 equally sized bins.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Software products make default choices for the value range and bin width.\n",
    "Typically the value range is taken to be the range of the data\n",
    "and equally spaced bins are used. There are several formulas for the\n",
    "number of bins which yield 'ideal' results under certain assumptions, \n",
    "in particular $\\sqrt{n}$ (Excel) and $3.5 \\sigma/n^{1/3}$ (Scott's rule) (cf. [Wikipedia](http://www.wikiwand.com/en/Histogram)).\n",
    "In practice, these choices do not yield satisfying results when applied\n",
    "to operations data like request latencies that contain many outliers.\n",
    "\n",
    "For readers familiar with probability theory, we mention that histograms are an estimator for the probability density function (cf. [Izenman  - Modern Multivariate Statistical Techniques, Springer, 208]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms as aggregation method\n",
    "\n",
    "When measuring high frequency data like, IO latencies, that\n",
    "can arive at a rates of more than 1.000 samples per sec, it\n",
    "is no longer feasilble to store all individual samples. \n",
    "If we are willing to forget about ordering and sacrifice\n",
    "some accuracy we can save massive amounts of space,\n",
    "by using histogram datastructures.\n",
    "\n",
    "The essential idea is, instead of storing the indiviudal samples\n",
    "as a list, to use the vector of bin counts that occures as\n",
    "an intermediate result in the histogram computation. E.g.\n",
    "in the above example in Listing 1, arrived at the following values:\n",
    "\n",
    "```python\n",
    "sample_count = [0, 10, 8, 4, 25, 23, 4, 2]\n",
    "```\n",
    "\n",
    "The precise memory representation used for storing histograms does vary.\n",
    "The important point is that we have the sample count of each bin available.\n",
    "\n",
    "Histograms allow approximate computation of various summary \n",
    "statistics, like mean values and quantiles that we will meet,\n",
    "in the following episodes. The acceived percision is dependent\n",
    "on the bin sizes.\n",
    "\n",
    "Also histograms can be aggregated easily. If you have\n",
    "request latencies available for each node of a database cluster\n",
    "in histograms with the same bin choices, then you can derive \n",
    "the latency distribution of the whole cluster by adding the \n",
    "sample counts for each bin. We can use the aggregated histogram\n",
    "to calculate mean values and quantiles over the whole cluster.\n",
    "\n",
    "This is in contrast to the situation when we computed mean values\n",
    "or quantiles for the nodes individually. It is not possible to\n",
    "derive, e.g. the 99%-percentile of the whole cluster from\n",
    "the 99%-percentiles of the individual nodes\n",
    "(cf. [Theo Schlossnagle - The Problem with Math](http://www.circonus.com/problem-math/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Dynamic Range Histograms\n",
    "\n",
    "High Dynamic Range (HDR) histograms provide a pragmatic choice for bin-width that allows\n",
    "a memory efficient representations suitable to captureing data on\n",
    "a very wide range that is common to machine generated data \n",
    "like, e.g. IO latencies. At the same time HDR histograms tend\n",
    "to produce acceptable visual representations in practice.\n",
    "\n",
    "HDR histograms change the bin width dynamically over the value range.\n",
    "A typical HDR histogram has a bin size of 0.1 between 1 and 10, \n",
    "with bin boundaries: $1,1.1,1.2,\\dots,9.9,10$. Similaraly between\n",
    "between 10 and 100 the bin size is 1, with boundaries $10,11,12,\\dots,100$.\n",
    "This pattern is repeasted for all powers of ten, so that we \n",
    "have 90 bins between $10^k$ and $10^{k+1}$. Typical range of $k$\n",
    "are $-127\\dots128$, often an extra bin for the $0$-value is appended.\n",
    "\n",
    "The general definition is a little bit more complex and lengthy,\n",
    "so that we don't give it here. The interested reader is referred \n",
    "to [http://hdrhistogram.org/] for more details and a memory-efficient\n",
    "implementation.\n",
    "\n",
    "From the above description it should be apparent, that\n",
    "HDR histograms span an extremly large value range ($[10^{-127}, 10^{128})$).\n",
    "The bin-sizes are very similiar to the float number precisions: \n",
    "The larger the value, the less precision is available. In addition\n",
    "the bin boundaries are independent on the dataset. Hence the aggregation\n",
    "technique described in the last paragraph applies to HDR histograms.\n",
    "\n",
    "### Histograms as Heatmaps\n",
    "\n",
    "If we are interested in observing the change of data distributions over time,\n",
    "we have to add an additional dimension to the histogram plot. \n",
    "A convenient method to do so, is represent the sample densities as a heatmap\n",
    "instead of a bar chart. Figure zz shows the request rate data visualized\n",
    "in such a way. Light colors mean low sample density, dark colors signal\n",
    "high sample desnity.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/histogram_heatmap_2.png\" />\n",
    "<figcaption>Figure zz: Request rate histogram (50 bins) preseted as heatmap.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can combine multiple such histogram heatmaps that were captured over time\n",
    "to a single two-dimensional heatmap. \n",
    "\n",
    "\n",
    "**War story:** \n",
    "Figure #heatmap shows a particularly interesting example of such a visualization for a sequence\n",
    "of (HDR) histograms of web request latencies. Note that the distribution of the data is bi-modal\n",
    "with one mode constant around `~5 ms` and another more diffuse mode ascending from `~10ms` to `~50ms`.\n",
    "In this particular case the second mode was caused by a bug in a session handler, that caused\n",
    "the addition of new entries to an in memory list. The particular list had to be traversed for\n",
    "each incoming request causing extended delays. Looking carefully, even the logarithmic growth\n",
    "of the average traversal time can be spotted.\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"img/Circonus_Histogram.png\"/></center>\n",
    "<figcaption>Figure #heatmap: Request latency heatmap over time in Circonus.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 3 -- Classical Summary Statistics\n",
    "\n",
    "The aim of summary statistics is to provide a summary of\n",
    "the essential features of a dataset.\n",
    "It is the numeric equivalent of an elevator pitch in a business context. If you just want the essential information, you\n",
    "should not need to look at all the details.\n",
    "\n",
    "A good summary statistic should be able to answer questions\n",
    "like \"What are typical values?\" or \"How much variation is in\n",
    "the data?\". A desireable property is robustnes against outliers.\n",
    "A single faulty measurement should not change a rough description\n",
    "of the dataset.\n",
    "\n",
    "In this episode we will discuss the classical summary statistics:\n",
    "mean values and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Value\n",
    "\n",
    "The _mean value_ or _average_ of a dataset $X=[x_1, \\dots, x_n]$ is defined as\n",
    "\n",
    "$$ mean(x_1, \\dots, x_n) = \\frac{1}{n} \\sum_{i=1}^n x_i. $$\n",
    "\n",
    "or when expressed as python code:\n",
    "\n",
    "```python\n",
    "def mean(X): return sum(X) / len(X)\n",
    "```\n",
    "\n",
    "The mean values has the physical interpretation of the center of\n",
    "mass if we place weights of equal weight on the points $x_i$\n",
    "on a (massless) axes. When the values of $x_i$ are close together,\n",
    "the mean value is a good representative of a typical sample.\n",
    "Contrary, when the samples are concentrated at several centers,\n",
    "or outliers are present, the mean value can be far from each\n",
    "individul datapoint (cf. Figure yy).\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/rugplot_mean.png\">\n",
    "<figcaption>Figure yy: Rugplot of a two-modal dataset (blue) with mean value (red).</figcaption>\n",
    "</figure>\n",
    "\n",
    "Mean values are abundant in IT operations.\n",
    "One common application of mean values is data rollup.\n",
    "When multiple samples arrived during a sampling period of e.g. one minute, the mean value is calculated as a  \"one-minute rollup\" and\n",
    "stored instead of the original samples. Similarly, if we have\n",
    "data available for every minute, but are only interested in\n",
    "hour intervals, we can \"rollup the data by the hour\", by taking mean values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Erosion\n",
    "\n",
    "When viewing metrics as line-plots in a monitoring system a phenomenon we\n",
    "call 'spike erosion' can often be observed.\n",
    "\n",
    "To reproduce this phenomeon, pick a metric (e.g. ping latencies) that experiences spikes at\n",
    "discrete points in time, and zoom in on one of those spikes and read off the height off\n",
    "the spike at the y-axis. Now zoom out of the graph to a range of\n",
    "one month, and read off the height of the same spike again.\n",
    "Are they equal? \n",
    "\n",
    "Figure #spikeerosion shows an example for such a graph. The spike height has decreased from `0.8` to `0.35`.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/spike_erosion.png\">\n",
    "<figcaption>Figure #spikeerosion: Ping latency spike on a view range of 2h vs. 48h.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "How is that possible? The result is an artifact of a rollup procedure\n",
    "that is commonly used when displaying graphs over long time ranges.\n",
    "The amount of data gathered over the period of one months (i.e. `>40K` minutes) is larger than the amount of pixels available for the plot.\n",
    "Therefore the data has to be rolled-up to larger time periods before\n",
    "it can be plotted. When the mean value is used for the rollups,\n",
    "the single spike is averaged with an increasing number of 'normal' samples and hence decreases in height.\n",
    "\n",
    "How to do better? The immediate way to address this problem is to choose an alternative rollup method, \n",
    "like max-values. However, one sacrifices information about typical values in this way.\n",
    "Another, more elegant, solution is to rollup values as histograms,\n",
    "and display a 2d heatmap instead of a line plot for larger view\n",
    "ranges. Both methods are illustrated in the third colum of Figure #spikeerosion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deviation Measures\n",
    "\n",
    "Once the mean value $\\mu$ of a dataset has been established, the next\n",
    "natural question is to measure the deviation of the indivudial samples\n",
    "from the mean value. The following three deviation measures are often \n",
    "found in paracice.\n",
    "\n",
    "The _maximal deviation_ is defined as\n",
    "\n",
    "$$ maxdev(x_1,\\dots,x_n) = max \\{ |x_i - \\mu| \\,|\\, i=1,\\dots,n\\},$$\n",
    "\n",
    "and gives an upper bound for the distance to the mean in the dataset.\n",
    "\n",
    "The _mean absolute deviation_ is defined as\n",
    "\n",
    "$$ mad(x_1,\\dots,x_n) = \\frac{1}{n} \\sum_{i=1}^n |x_i - \\mu|$$\n",
    "\n",
    "and is the most direct mathematical translation of a \"typical deviation\" from the mean.\n",
    "\n",
    "The _standard deviation_ is defined as\n",
    "\n",
    "$$ stddev(x_1,\\dots,x_n) =  \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2}.$$\n",
    "\n",
    "While the intuition behind this definition is not obvious, this deviation measure is very popular for it's nice mathematical properties (as being derived from a quadratic form). In fact, all three of the above deviation measures fit into a continues family of $p$-deviations (cf. http://www.wikiwand.com/en/Lp_space), which feature the standard deviation in a 'central' position.\n",
    "\n",
    "Figure #devmeasures shows the mean value and all three deviation measures for a request latency dataset.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"img/deviation_measures.png\">\n",
    "    <figcaption>\n",
    "    Figure #devmeasures: A request latency dataset with mean value (red) mean absolute deviation (top), standard deviation.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "We immediately obeserve the following inequalities\n",
    "\n",
    "$$ mad(x_1,\\dots,x_n) \\leq stddev(x_1,\\dots,x_n) \\leq maxdev(x_1,\\dots,x_n). $$\n",
    "\n",
    "It can be shown that this relation holds true in general.\n",
    "\n",
    "We also see that the the presence of ourliers affects all three deviation measures significantly.\n",
    "The maximal deviation is even larger than the mean value and thus represents possible deviations of the dataset into the negative value range, which is impossible for request latencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caution with the Standard Deviation\n",
    "\n",
    "Many of use remember the following rule of thumb from school:\n",
    "\n",
    "- 68% of all samples lie within one standdard deviations of the mean, \n",
    "- 95% of all samples lie within two standdard deviations of the mean,\n",
    "- 99.7% of all samples lie within three standdard deviations of the mean.\n",
    "\n",
    "This assertions rely on the crucial assumption that the data is normaly distributed. For operations data this almost never the case, and the rule fails quite drastically: In the above example more than 0.97% lie between within one standdard deviation of the mean value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following war story is can be found in [Janert - Data Analysis with Open Source Tools]:\n",
    "\n",
    "A service level agreement (SLA) for a database defined a latency outlier as a value outside of 3 standard deviations.\n",
    "The programmer that implemented the SLA check, remembered \n",
    "the above rule naively and computed the latency of the slowest\n",
    "0.3% of the queries instead.\n",
    "\n",
    "This rule has little to do with the orginal defintion in practice.\n",
    "In particular this rule labels 0.3% of each dataset blindly \n",
    "as outliers. Moreover, it turned out that the reported value\n",
    "captured long running batch jobs that were in the order of hours.\n",
    "Finally the programmer hard-coded some seemingly reasonable\n",
    "threshold value of ~50 seconds, and that was reported as the \"3 standard deviations\" regardless of the actual input.\n",
    "\n",
    "The actual SLA was never changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 4 -- Quantiles and Outliers\n",
    "\n",
    "The classical summary statistics introduced in Episode 3 are\n",
    "well suited for describing homogenous distributions, but\n",
    "are easily affected by outliers. Moreover, they do not\n",
    "contain much information about the tails of the distribution.\n",
    "\n",
    "Qunatiles are a flexible tool that offers an alternative to\n",
    "the classical summary statistics that is less susceptible\n",
    "to outliers.\n",
    "\n",
    "### Comulative Distribution Functions\n",
    "\n",
    "Before we can introduce quantiles, we need to recall the following concept.\n",
    "The (empirical) cumulative distribution function  $CDF(y)$ for dataset $X$,\n",
    "at a vale $y$ is the ratio of samples that are lower than the value $y$:\n",
    "\n",
    "$$ CDF(X,y) = \\# \\{ i \\, | \\, x_i \\leq y \\} / \\# X $$\n",
    "\n",
    "Or expressed in python code:\n",
    "```python\n",
    "def CDF(X,y):\n",
    "    lower_count = 0\n",
    "    for x in X: if x <= y: lower_count += 1\n",
    "    return float(lower_count) / len(X)\n",
    "```\n",
    "\n",
    "Figure #CDF shows an example for a dataset of request rates.\n",
    "Note $CDF(X,y)$ takes values between $0$ and $1$ and is monotonically increasing as a function of $y$.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/cdf.png\" width=\"50%\">\n",
    "<figcaption>Figure #CDF: The comulative distribution function for a data set of request rates.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiles and Percentiles\n",
    "\n",
    "Fix a number $q$ between $0$ and $1$ and a dataset $X$ of size $n$.\n",
    "Roughly speaking, a $q$-quantile is a number $y$ that divides $X$ into two sides, with a ratio of $q$ samples lying below $y$ and \n",
    "the remaining ratio of $1-q$ samples lies above $y$.\n",
    "\n",
    "More formally, a $q$-quantile for $X$ is a value $y$ so that:\n",
    "1. at least $q \\cdot n$ samples that are lower or equal than $y$ and\n",
    "2. at least $(1-q) \\cdot n$ samples that are larger or equal to $y$. \n",
    "\n",
    "Familiar examples are the minimum, which is a a 0-quantile, the maximum,\n",
    "which is a 1-qunatile, and the median, which is a 0.5-quantile, by definition.\n",
    "Common names for special quantiles include _percentiles_ for $k/100$-qunatiles\n",
    "and _quartiles_ for $k/4$-quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that quantiles are not unique. There are ways to make quantiles unique,\n",
    "but those involve a choice that is not obvious to take. [Wikipedia](https://www.wikiwand.com/en/Quantile)\n",
    "lists nine different choices that are found in common software products.\n",
    "So if people talk about _the_ q-quantile, or _the_ median, one should \n",
    "always be careful and question which choice was made.\n",
    "\n",
    "As a simple example of how quantiles are non unique, take a dataset\n",
    "with two values `X = [10,20]`. Which values are medians, $0$-quantiles,\n",
    "$0.25$-quantiles? Try to figure it out yourself.\n",
    "\n",
    "The good news is, that $q$-quantiles do always exists and are easy to compute.\n",
    "Indeed, let $S$ be a sorted copy of the data set $X$ so that the smalles element\n",
    "$X$ is equal to $S[0]$ and the largest element of $X$ is equal to $S[n-1]$.\n",
    "If $d=\\text{floor}(q \\cdot (n-1))$, then $S[d]$ will have $d+1$ samples\n",
    "$S[0],\\dots,S[d]$, which are are smaller or equal to $S[d]$ and $n-d+1$ \n",
    "samples $S[d], .., S[n]$ wich are larger or equal to $S[d]$. \n",
    "It follows that $S[d]=y$ is a $q$-quantile. The same argument holds true\n",
    "for $d=\\text{ceil}(q \\cdot (n-1))$.\n",
    "\n",
    "The following listing gives an python implementation of the above construction:\n",
    "\n",
    "```python\n",
    "def quantile_range(q,X):\n",
    "    S = sorted(X)\n",
    "    n = len(X)\n",
    "    return (\n",
    "      S[int(math.floor(q * (n - 1)))], \n",
    "      S[int(math.ceil(q * (n - 1)))]\n",
    "    )\n",
    "```\n",
    "\n",
    "It is not hard to see, that the above construction are the minimal and the\n",
    "maximal possible $q$-quantiles. We introduce the notation $Q_{min}(X,q)$, for\n",
    "the minimal $q$-quantile. The minimal quantile has the proprty that \n",
    "$Q_{min}(X,q) \\leq y$ if and only if at least $n \\cdot q$ samples of $X$ \n",
    "are lower or equal than $y$. A similar statment holds true for the maximal\n",
    "quantile when checking ratios of samples that are at larger than $y$.\n",
    "\n",
    "Quantiles are closely related to the Comulative Distribution functions\n",
    "discussed in the last section. Those concepts are inverse to each other\n",
    "in the following sense: If $CDF(X,q) = q$, then $y$ is a $q$-quantile for $X$.\n",
    "Because of this property, we also refer to 'Comulative Distribution Function values'\n",
    "as _inverse quantiles_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications to Service Level Monitoring\n",
    "\n",
    "Quantiles and CDF's give a powerfull method to measure service levels.\n",
    "To see how this works, consider the following service level agreements (SLA) \n",
    "that is still commonly seen in practice:\n",
    "\n",
    "> The mean response time of the service shall not exceed 3ms,\n",
    "> when measured each minute over the course of one hour.\n",
    "\n",
    "This SLA does not capture the experience of the service consumers well.\n",
    "First, the requirement can be violated by a single request that took more\n",
    "than a 90ms to complete. Also, a long period, where low overall load\n",
    "caused the measured request to finish within 0.1ms, can compensate for\n",
    "a short period where a lot of external requests were serviced with\n",
    "unacceptable response times of 100ms or more.\n",
    "\n",
    "An SLA that captures the quality of service as experienced by the \n",
    "cusomters, looks as follows:\n",
    "\n",
    "> 80% of all requests served by the API within one hour should complete within 3ms.\n",
    "\n",
    "Not only is this SLA easier to formulat, it also avoids the above problems:\n",
    "A single long running request does not violate the SLA, and a busy period with long\n",
    "response times will violate SLA, if more than 20% of all queries are affected.\n",
    "\n",
    "In order to check the SLA, we give two equivalent formulations in terms of quantiles and CFD's:\n",
    "\n",
    "* The minimal $0.8$-quantile is at most 3 ms: $Q_{min}(X_{1h}, 0.8) \\leq 3ms$.\n",
    "\n",
    "* The 3ms-inverse quantile is larger than 0.8: $CDF(3ms, X_{1h}) \\geq 0.8$.\n",
    "\n",
    "Here $X_{1h}$ denotes the samples that lie within a 1h window. Both formuations can be used to monitor service levels effectively. Figure #QT shows a plot of $Q_{min}(X_{1h}, 0.8)$ as a line plot. Note how on the 24th July,\n",
    "the quantile raises above 3ms, indicating a violation of the SLA.\n",
    "Figure #IVP shows a plot of the inverse quantile $CDF(3ms, X_{1h})$, which takes values on the right axis \n",
    "between 0% and 100%. The SLA violation manifests by the, inverse quantile dropping below 80%.\n",
    "\n",
    "Hence, quantiles and inverse quantiles give a complementary view towards the current service level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"img/quantile_example.png\">\n",
    "<figcaption>Figure #IVP: Histogram metric with $Q_{min}(0.8)$ over 1h windows</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/ivp_example.png\">\n",
    "<figcaption>Figure #IVP: Histogram metric with CFD(3ms) over 1h windows</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this article we presented an overview about some statistical techniques that find applications in IT Operations.\n",
    "We discussed several visualization methods, their qualities and relations to each other.\n",
    "We demonstrated how histograms are an effective tool to capture data and visualize sample distributions.\n",
    "Finnaly we introduced the concepts of percentiles and inverse percentiles and discussed their application to\n",
    "monitor service levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## About the Author\n",
    "\n",
    "Heinrich Hartmann is Chief Data Scientist for Circonus. He earned his\n",
    "PhD in pure Mathematics from the University of Bonn (Germany) on\n",
    "geometric aspects of string theory and worked as a researcher for the\n",
    "University of Oxford (UK) afterwards.  In 2012 he transitioned into\n",
    "computer science and worked as independent consultant for a number of\n",
    "different companies and research institutions. He is now leading the\n",
    "development of data analytics for the Circonus monitoring product.\n",
    "\n",
    "## About Circonus\n",
    "\n",
    "Circonus provides analytics and monitoring for Web-Scale IT. Developed\n",
    "specifically for the requirements of DevOps, the Circonus platform\n",
    "delivers alerts, graphs, dashboards and machine-learning intelligence\n",
    "that help to optimize not just your operations, but also your\n",
    "business. Proprietary Database technology and Analytics tools enable\n",
    "Circonus to provide forensic, predictive, and automated analytics\n",
    "capabilities that no other product can match, and at a scale that\n",
    "other products can only dream of.\n",
    "\n",
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TBD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
