{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for Engineers\n",
    "\n",
    "by Heinrich Hartmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Statistics plays a great role in modern IT operations.\n",
    "Monitoring systems collect a wealth of data from\n",
    "network-gear, operating system, application level metrics,\n",
    "that needs to be analyzed to derive vital information.\n",
    "For example, faults need to be detected early, we want\n",
    "to measure the performance the product or forecast\n",
    "the ressources that we will need to serve our users\n",
    "next month.\n",
    "\n",
    "Statistics is the art of extracting information from data.\n",
    "Hence, it becomes key to operate a modern IT system. \n",
    "Despite the community becoming more and more aware of this\n",
    "fact, ressources for learning the relevant statsitics for\n",
    "this domain are hard to find. \n",
    "\n",
    "In particular classical statistics appears not to be adequarte.\n",
    "Statistics courses at university depends on a great amount of\n",
    "pre-knowledge in probablity-, measure- and set-theory, which\n",
    "is very hard to digest. Moreover, it often focusses on \n",
    "parametric methods, like t-test that come with strong \n",
    "assumtptions on the distribution of the data ('normality')\n",
    "that are not met by operations data.\n",
    "\n",
    "The focus of many statistical treatments is largely outdated. The origins of statistics reach back to the 17-century, where computation was very expensive and data was a very sparse ressource. So mathematicians spent a lot of effort at avoiding calculations. The setting has has changed radically and allows different approaches to statistical problems.\n",
    "\n",
    "Have a look at this example form the book [H.O. Georgii - Stochastik, DeGruyter, 2002] used at my\n",
    "statistics class at university:\n",
    "\n",
    "> An furit-merchant gets a delivery of 10.000 oranges.\n",
    "> He want's to know how many of those are rotten.\n",
    "> To do so he takes a sample of 50 oranges, and counts the\n",
    "> numer of rotten ones x? Which deductions can he make\n",
    "> about the total number of rotten oranges?\n",
    "\n",
    "The chapter goes on with explaining various infrence methods.\n",
    "The example transated to our domain would go as follows:\n",
    "\n",
    "> A db admin want's to know how many requests took longer\n",
    "> than 1second to complete. He measures the duration of all\n",
    "> requests and count's the number of those which took longer\n",
    "> than 1second. Done.\n",
    "\n",
    "Wow that was easy. No statistics needed at all!\n",
    "\n",
    "Therefore, we will to take a different approach to Statistics\n",
    "in this article. Instead of presenting text book material, I'll\n",
    "present 5 episodes of relevant descriptive statistical methods\n",
    "that are accessible and relevant for case in point. I have\n",
    "tried to keep the mathematical prior knowledge to a minimum,\n",
    "by e.g. replacing formulas by source code whenever feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 1: Visualizing Data\n",
    "\n",
    "The most essential data analysis method is visualization.\n",
    "The human brain can process geomeric information much more rapidly than numbers or language. When presented with a suitable\n",
    "visualization we can capture relevant properties, like\n",
    "typical values and outliers almost instantly.\n",
    "\n",
    "In this episode we will run through the basic plotting\n",
    "methods and discuss their properties. For producing the\n",
    "plots, we have chosen to use the python toolchaing\n",
    "([ipython](http://ipython.org), [matplotlib](http://matplotlib.org), and [seaborn](http://stanford.edu/~mwaskom/software/seaborn/)).\n",
    "We will not show you, however, how to use this tools.\n",
    "There are a lot of alternative plotting tools (R, MATLAB) with\n",
    "accompaniating tutorials available online.\n",
    "Source code an Datasets can be found on <a href=\"http://github.com/HeinrichHartmann/Statistics-for-Engineers\">\n",
    "GitHub.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rugplots\n",
    "\n",
    "The most basic visualization method of a one-dimensional dataset \n",
    "`X = [x_1, ... ,x_n]`\n",
    "is the rugplot (cf. Figure 1). A rugplot consists of a single axis on which little lines, called 'rugs' are drawn for each sample `x_i`.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_rugplot_3.png\">\n",
    "<figcaption>Figure 1: A Rugplot of web-requests rates</figcaption>\n",
    "</figure>\n",
    "\n",
    "Rugplots are suitable for all questions where the ordering of the samples is not relevant, like common values or outliers.\n",
    "Problems occure if there are multiple samples with the sampe\n",
    "value in the dataset. Those samples will be indistinguishable\n",
    "in the rugplot. This problem can be addressed by adding a\n",
    "small random displacement (jitter) to the samples.\n",
    "\n",
    "Despite it's simple and honest character, the rugplot is not very\n",
    "commonly used in practice, instead histograms or line plots. The author thinks that this should change!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Histograms are a popular visualization method for unordered\n",
    "one-dimensional data. Instead of drawing rugs on an axis,\n",
    "the we divide the axis into bins, and draw bars of a\n",
    "ceratin height on top of them, so that the number of\n",
    "samples within a bin is proportional to the area of the bar (cf. Figure 2).\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_histogram_2.png\">\n",
    "<figcaption>Figure 2: Histogram</figcaption>\n",
    "</figure>\n",
    "\n",
    "The use of a second dimension makes the histogram in\n",
    "many cases easier to comprehend than a rugplot. In\n",
    "particular, questions like: \"Which ratio of the samples\n",
    "lies below y?\" can be effecifly estimated by comparing\n",
    "areas. This convenience comes at the expenese of an\n",
    "extra dimension used and \n",
    "\n",
    "There is a lot more to tell about histograms, and our next\n",
    "episode is entirely devoted to this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot\n",
    "\n",
    "The scatterplot is the most basic visualization of\n",
    "a two-dimensional dataset. For each pair `x,y` of values\n",
    "we draw a point on a canvas, that has coordinates `(x,y)`\n",
    "in a cartesian coordinate system.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_scatterplot_2_1.png\" width=\"45%\" style=\"float:left\">\n",
    "<img src=\"img/example_scatterplot_2_2.png\" width=\"45%\">\n",
    "<figcaption>Figure 3: Scatterplots of request rates of two database nodes.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Scatterplot are a great tool to compare repeated measurments of\n",
    "two different quantities. In Figure 3 we have plotted the\n",
    "request rates of two different database nodes in a scatterplot.\n",
    "On the left side, the points are mainly concentrated on a diagonal line, which means that if one node servers a lot of requests then the other one is doing so as well.\n",
    "On the right side, the points are scattered all over the canvas,\n",
    "which represents a highly irregular load distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Plots\n",
    "\n",
    "Line plots are by far the most popular visualization method seen\n",
    "in practice. It is a special case of a scatter plot, where\n",
    "timestamps are plotted on the x-axis. In addition a line\n",
    "is drawn between consecutive points. This addition \n",
    "provides the impression of a continues transition\n",
    "between the individual samples.\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_lineplot.png\">\n",
    "<figcaption>Figure 4: A line plot of web-request rates.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Line plots are a great tool to surface time dependent patterns\n",
    "like periods or trends. When you are interested in\n",
    "time-independent question, like typical values, other methods\n",
    "like rug-plots might be more better suited.\n",
    "\n",
    "When viewing a line plot the continuity of the underlying data should always be challenged. Just because the CPU was idle \n",
    "at 1:00pm and one minute later, it does not mean it did not\n",
    "do any work in between.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one to use?\n",
    "\n",
    "Start your analysis with a particular question that your have?\n",
    "\n",
    "* Lineplots are a special case of scatterplots\n",
    "* Scatterplot has marginal rugplots\n",
    "* Lineplot has marginal y-rugplot\n",
    "* Histograms show the same information as rugplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 2 -- Histograms\n",
    "\n",
    "- we will talk about histograms as visualization method, and as a data storage format\n",
    "- fundamental visualization of one-dimensional, unordered data\n",
    "- can compare areas instead of counts in rugplots\n",
    "\n",
    "\n",
    "To understand all the details, let's build our own histogram.  The\n",
    "first thing in building a histogram is to choose a range of values\n",
    "that should be covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_min = 500\n",
    "bin_max = 2200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to partition the value range into bins. Bins are often\n",
    "equaly spaced but there is no need to follow this convention.  We\n",
    "represent the bin partition by a sequence of bin-boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [bin_min, 700, 800, 900, 1000, 1500, 1800, 2000, bin_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first bin, extends from `500` to `700` and the last bin from\n",
    "`2000` to `2200`.  (By convention, the boundary values are considered\n",
    "part of the upper bin.)\n",
    "\n",
    "Now for a given dataset `X` we have to count how many samples are\n",
    "contained in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_count(bins, X):\n",
    "    bin_count = len(bins) - 1\n",
    "    counts = [0] * bin_count\n",
    "    for x in X:\n",
    "         for bin_idx in range(bin_count):\n",
    "            if (bins[bin_idx] <= x) and (x < bins[bin_idx + 1]):\n",
    "                counts[bin_idx] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we have to produce a bar-chart, where each bar is based on one bin,\n",
    "and the bar-height is equal to sample count divided by bin width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_histogram(bins, X):\n",
    "    bin_widths = [ float(bins[i] - bins[i-1])\n",
    "                   for i in range(1,len(bins)) ]\n",
    "    bin_heights = [ count/width\n",
    "                    for count, width\n",
    "                    in zip(sample_count(bins, X), bin_widths) ]\n",
    "    bin_lefts = bins[:-1] # skip last value; no bucket left to the maximum\n",
    "    plt.bar(bin_lefts, width=bin_widths, height=bin_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.genfromtxt(\"DataSets/RequestRates.csv\", delimiter=\",\")[:,1]\n",
    "plot_histogram(bins, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"img/histogram_manual_1.png\">\n",
    "<figcaption>Example Line Plot\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "So let's note a few things:\n",
    "- Producing histograms is not hard.\n",
    "- There are some choices involved: range, bins\n",
    "- The choices can change the visual appearance quite a bit\n",
    "\n",
    "Example:\n",
    "- small bin-width, looks like a rug-plot\n",
    "- just one bin -> just one bar of height sample_count / range\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/histogram_manual_2.png\">\n",
    "<figcaption>Example Line Plot\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Common choices found in the literature are \n",
    "use max and min for the range and, equally spaced bins\n",
    "of size $\\sqrt{n}$ (Excel), $\\frac{3.5 \\sigma}{n^{1/3}}$ (Scott's rule)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A pragmatic choice: HDR Histograms\n",
    "\n",
    "Example: A decimal HDR Histogram with precision=2 has bin boundaries:\n",
    "  \n",
    "    .... 1.0,  1.1,  1.2, ... 1.9,  2.0,  2.1, ...., 9.9; --- bin width = 0.1\n",
    "         10.0, 11.0, 12.0 ... 19.0, 20.0, 21.0 ...., 99 ; --- bin width = 1\n",
    "         ...\n",
    "\n",
    "** Properties: **\n",
    "* Captures large part of float range\n",
    "* Bin boundaries do not depend on data! -> Can aggregate counts!\n",
    "* Bin width increase with growing values\n",
    "* Allows compact memory representation\n",
    "* Implementation available at http://hdrhistogram.org/\n",
    "\n",
    "IMAGE of Circonus HDR Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 3 -- Summary Statistics\n",
    "\n",
    "Problem: Given a bunch of datapoints:\n",
    "\n",
    "* characterize the distribution in one or two values\n",
    "* Characterization should be robust to outliers\n",
    "\n",
    "Equivalent of an elevator pitch for a data sets.\n",
    "\n",
    "Problem: This is inherently impossible\n",
    "\n",
    "## Mean Value\n",
    "\n",
    "The _mean value_ of $x_1, \\dots, x_n$ is defined as\n",
    "\n",
    "$$ \\mu = mean(x_1, \\dots, x_n) = \\frac{1}{n} \\sum_{i=1}^n x_i. $$\n",
    "\n",
    "- Represnets center of mass\n",
    "- If the values are close together this is a good representative\n",
    "\n",
    "Problems with mean values:\n",
    "- can be far away from all datapoints\n",
    "- sensitive to ourliers\n",
    "\n",
    "## Peak Erosion\n",
    "\n",
    "* A monitoring graph rarely shows you the full data: Not enough pixels!\n",
    "* Need to choose an summary statistic to pre-aggregate the data.\n",
    "* Common choice: mean\n",
    "\n",
    "IMAGE Spikes\n",
    "\n",
    "## Deviation Measures\n",
    "\n",
    "1. The _maximal deviation_ is defined as\n",
    "\n",
    "$$ maxdev(x_1,\\dots,x_n) = max \\{ |x_i - \\mu| \\,|\\, i=1,\\dots,n\\}.$$\n",
    "\n",
    "2. The _mean absolute deviation_ is defined as\n",
    "\n",
    "$$ mad(x_1,\\dots,x_n) = \\frac{1}{n} \\sum_{i=1}^n |x_i - \\mu|.$$\n",
    "\n",
    "3. The _standard deviation_ is defined as\n",
    "\n",
    "$$ \\sigma = stddev(x_1,\\dots,x_n) =  \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2}.$$\n",
    "\n",
    "\n",
    "* Measure the 'typical' displacement from the mean value.\n",
    "* Standard deviation is popular because it has extremely nice mathematical properties.\n",
    "* Part of a continues family of deviation measures (p-norms).\n",
    "\n",
    "Remarks:\n",
    "- Standard deviations are very sensitive to ourliers\n",
    "- Mean deviation is better but still linear impact on outliers.\n",
    "\n",
    "\n",
    "IMAGE\n",
    "\n",
    "## Caution with Standard Deviation\n",
    "\n",
    "- Everybody Learns about standard deviation in school\n",
    "- Beautiful mathematical properties!\n",
    "- Everybody knows \n",
    "  - \"68% of data falls within 1 std-dev of the mean\"\n",
    "  - \"95% falls within 2 std-dev of the mean\"\n",
    "  - \"99.7\" falls within 3 std-dev of the mean\"\n",
    "* \"Problem is: this is utter nonsense\". Only true for normally distributed data.\n",
    "\n",
    "* Not good for measuring outliers!\n",
    "\n",
    "\n",
    "_Source:_ Janert - Data Analysis with Open Source Tools\n",
    "\n",
    "## War Story:\n",
    "\n",
    "- Looking at SLA for DB response times\n",
    "- Outlier defined as value larger than $\\mu + 3\\sigma$\n",
    "- Look at code: Takes '0.3' percentile!\n",
    "- So always have outliers.\n",
    "- And 0.3-percentile was way too large (hours of latency).\n",
    "- Programmer changed code for 1%, 5%, 10% quantiles.\n",
    "- Finally handcoded a threshold\n",
    "- The SLA was never changed\n",
    "\n",
    "Source: Janert - Data Analysis with Open Source Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 4 -- Quantiles and Outliers\n",
    "\n",
    "* Classical summary statistics good for describing the body of the distribution\n",
    "* Need information about the tail of the distributions, e.g. for writing good SLAs\n",
    "* Determine outliers in a dataset\n",
    "\n",
    "### Comulative Distribution Functions\n",
    "\n",
    "The (empirical) cumulative distribution function of a dataset $X$, is defined as:\n",
    "\n",
    "$$ CDF(y) = \\# \\{ i \\, | \\, x_i \\leq y \\} / \\# X $$\n",
    "\n",
    "So CDF(y) = the ratio of samples that are lower than $y$.\n",
    "\n",
    "IMAGE\n",
    "\n",
    "Properties:\n",
    "\n",
    "* $0 \\leq CDF(y) \\leq 1$\n",
    "* CDF is monotonically increasing\n",
    "\n",
    "## Quantiles and Percentiles\n",
    "\n",
    "* Complement or Inverse to CDFs:\n",
    "  - CDF: The ratio of samples was below 100 was `<CDF>`\n",
    "  - Quantile: 90% of all queries where faster than `<quantile>`\n",
    "* Qunatile parameter is indpendent of value range\n",
    "\n",
    "Examples:\n",
    "\n",
    "* The minimum is a 0-quantile\n",
    "* A median is a 0.5-quantile\n",
    "* The maximum is a 1-quantile\n",
    "\n",
    "Special names:\n",
    "\n",
    "* Quartiles: $k/4$-quantiles\n",
    "* Percentiles: $k/100$-quantiles\n",
    "\n",
    "### Median\n",
    "\n",
    "A _median value_ for $x_1, \\dots, x_n$ is number $m$ such that\n",
    "  \n",
    " $$ \\# \\{ i \\,|\\, x_i \\leq m \\} = \\# \\{ i \\,|\\, x_i \\geq m \\}. $$\n",
    "\n",
    "So the number of samples smaller than $m$ is equal to the number of samples larger than $m$.\n",
    "(Both should be roughly $n/2$).\n",
    "  \n",
    "Remark:\n",
    "\n",
    "* A Median always exists\n",
    "* Median is not unique\n",
    "* Can be computed in linear time\n",
    "* Not influenced by outliers (robust)\n",
    "\n",
    "\n",
    "### General Definition of Quantiles\n",
    "Let $0\\leq q \\leq 1$ be a real number. A $q$-quantile for $X$ is a value $y$ such that, if\n",
    "\n",
    "$$ \\#\\{i \\,|\\, x_i \\leq y \\} \\geq q \\cdot n  $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\#\\{i \\,|\\, x_i \\geq y \\} \\geq (1-q) \\cdot n $$\n",
    "\n",
    "Roughly speaking, $y$ divides $X$ in $q \\cdot n$ samples that are lower than $y$ and $(1-q) \\cdot n$ samples that are larger than $y$.\n",
    "\n",
    "Remarks:\n",
    "\n",
    "* Quantiles always exists\n",
    "* Non unique (like median)\n",
    "* Lot's of ways to choose a quantile function, i.e. interpolate between $s_a$ and $s_b$ cf.  \n",
    "  http://en.wikipedia.org/wiki/Quantile#Estimating_the_quantiles_of_a_population\n",
    "  \n",
    "### Using quantiles and CDF to monitor/formulate SLAs\n",
    "\n",
    "CDFs can be used to determine service levels.\n",
    "\n",
    "1. Measure response time latencies each minute over 1h\n",
    "2. Calculate `CDF(<max tolerable latency>)` for each 1h window\n",
    "3. Good service when `CDF = 1`\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_cdf_sla.png\">\n",
    "<figcaption>CDF(0.05) over 1h windows for Twitter Ping Latencies</figcaption>\n",
    "</figure>\n",
    "\n",
    "* Histograms are a much better representation of actual API usage\n",
    "* Calculate CDF over all requestst that arrived in 1h\n",
    "* Catch delays that affect only a small fraction of the requests\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_histogram_IVP.png\">\n",
    "<figcaption>Histogram metric with CFD(3) over 1h windows</figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IQR and Outliers\n",
    "\n",
    "The interquartile range of a sample X is defined as:\n",
    "\n",
    "`IQR(X) = Q(0.75,X) - Q(0.25,X)`\n",
    "\n",
    "It is a robust measure for variance of the data. Good alternative to standard / mean deviation.\n",
    "\n",
    "**Def.** (Tukey, 1969) a k-outlier is a data point X which is either\n",
    "\n",
    "* larger than `Q(0.75) + k * IQR(X)` or\n",
    "* smaller than `Q(0.25) - k * IQR(X)`.\n",
    "\n",
    "An outlier (without k) is an 1.5-outlier.\n",
    "\n",
    "# Tukey's Boxplots\n",
    "\n",
    "Show:\n",
    "\n",
    "* Median\n",
    "* Box around 0.25 and 0.75 Quantiles\n",
    "* \"whiskers\" from min to max\n",
    "* points for outliers\n",
    "\n",
    "Allows visual clues:\n",
    "\n",
    "* Where is the data concentracted?\n",
    "* How far is it spread?\n",
    "* How skew is the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 4b -- Comparing Distributions\n",
    "\n",
    "- Problem: Code change. Want to compare performance. Did it change?\n",
    "  -> For the better?\n",
    "  -> For the worse?\n",
    "- Classical T-Test has normal assumption in it\n",
    "- Use: Kolmogorov-Smirnov distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 5 -- Forecasting\n",
    "\n",
    "- Main use case forecast data for capacity planning\n",
    "- Regression can be used to capture linear and exponential trends\n",
    "- Holt-Winters method can be used to forecast periodic data \n",
    "  but many parameters, lot's of things can go wrong!\n",
    "  Need error estimates for this.\n",
    "  \n",
    "- Avoid forecasting complex behavior (if you can)\n",
    "- Use regression or robust regressions for forecasting\n",
    "- How well did we do? Evaluate using goodness of fit measure, R2.\n",
    "\n",
    "\n",
    "## Regression Method\n",
    "\n",
    "* Given two vectors `X,Y` of the same length\n",
    "* Q: Can we predict `Y[i]` from knowing `X[i]`\n",
    "* Can we find a (mathematical) function $f$ so that $f(x)$ is close to $y$?\n",
    "\n",
    "* Parametric Ansatz for $f(x) = f(\\theta; x)$\n",
    "* Define residuals $e_i$ by:\n",
    "\n",
    "$$x_i = f(\\theta; t_i) + e_i$$\n",
    "\n",
    "* Loss function:\n",
    "\n",
    "$$ Loss(\\theta) = \\sum_i e_i^2 = \\sum_i (y_i - f(\\theta;t_i))^2 $$ \n",
    "\n",
    "* Chosse $\\hat{\\theta}$ by minimizing $Loss(\\theta)$.\n",
    "\n",
    "* A _goodness of fit_ measure is the minimal loss $MinLoss=Loss(\\hat{\\theta})$\n",
    "\n",
    "## Warmup: Constant Model\n",
    "\n",
    "* Parametric Ansatz for f: $f(x) = a$, constant.\n",
    "* Quadratic loss function:\n",
    "\n",
    "$$ Loss(a) = \\sum_i (y_i - a)^2 $$\n",
    "\n",
    "* Minimize loss function (_using calculus!_) gives __mean value__:\n",
    "\n",
    "$$ \\hat{a} = \\frac{1}{n} \\sum_i y_i = \\mu_Y $$\n",
    "\n",
    "$$ MinLoss(X,Y) = \\sum_i (y_i - \\mu_Y)^2$$\n",
    "\n",
    "* This quantity is also known as ($n$ times) the __variance__. So:\n",
    "* $Var(Y)$ measures goodness of constant fit!\n",
    "\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "* Parametric Ansatz for f:\n",
    "  $$f(a,b; t) = bt + a, \\quad \\theta=(a,b)$$\n",
    "\n",
    "* Quadratic loss function:\n",
    "\n",
    "$$ Loss(a,b) = \\sum_i e_i^2 = \\sum_i (y_i - f(t_i))^2 =  \\sum_i (y_i - b t_i - a)^2 $$ \n",
    "\n",
    "$$ = a^2 A + ab \\cdot B + b^2 \\cdot C + D$$\n",
    "\n",
    "for some variables $A,B,C,D$ depending on $X,Y$.\n",
    "\n",
    "* Regression: Minimize $Loss(a,b)$ with respect to parameters $(a,b)$.\n",
    "\n",
    "* Since $Loss(a,b)$ is quadratic, it has a unique minimum which is easy to compute:\n",
    "\n",
    "$$\\hat{b} =  \\frac{\\sum_i (x_i - \\mu_x)(y_i - \\mu_y)}{\\sum_i(x_i - \\mu_x)^2} = Cov(X,Y) / Var(X) = \\rho_{X,Y} \\frac{\\sigma_Y}{\\sigma_X}$$\n",
    "\n",
    "$$\\hat{a} = \\mu_Y - \\hat{b} \\mu_X $$\n",
    "\n",
    "Remarks:\n",
    "\n",
    "* See (http://en.wikipedia.org/wiki/Simple_linear_regression) for a derivation.\n",
    "\n",
    "* Works also for more complex functions (e.g. polynomials)\n",
    "\n",
    "* Name \"linear\" regression comes from \"quadratic\" (!) loss function.\n",
    "\n",
    "* Note that Pearson Correlation appears in formula for $\\hat{b}$.\n",
    "\n",
    "\n",
    "\n",
    "## Exponential regression\n",
    "\n",
    "Use a different Model\n",
    "\n",
    "$$f(t) = exp(a \\cdot t + b)$$\n",
    "\n",
    "Trick: Use log to reduce to linear case.\n",
    "\n",
    "* Forecast exponential growth\n",
    "* Compute Compound Annual Growth rate\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/example_exp_forecast.png\">\n",
    "<figcaption>Exponential user-statistics forecast.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "## Goodness of fit\n",
    "\n",
    "* Natural measure for goodness of fit is the Loss of the ideal fit viewed as a function of $X$ and $Y$:\n",
    "\n",
    "$$ MinLoss(X,Y) = \\sum_i (y_i - \\hat{b} x_i - \\hat{a})^2 $$\n",
    "\n",
    "* Problem: Scales (quadratic) with $Y$: $MinLoss(X,3 Y)=9 MinLoss(Y)$ without the regression getting better or worse. \n",
    "\n",
    "* Use loss of constant regression (ie. mean value):\n",
    "\n",
    "$$ ConstLoss(Y) = \\sum_i (y_i - \\mu_y)^2  = n \\cdot Var(Y) $$\n",
    "  \n",
    "**Def:** The $R^2-Value$ value is\n",
    "\n",
    "$$ R^2 = 1 - \\frac{MinLoss(X,Y)}{ConstLoss(Y)}$$\n",
    "\n",
    "* Lies in [0,1].\n",
    "* Perfect fit if $R^2=1$\n",
    "\n",
    "* Scaling invariance: Does not change when $X,Y$ is replaced by $a X, b Y$.\n",
    "\n",
    "* Can be viewed as variance ratio $SS_{reg} / SS_{tot}$.\n",
    "  http://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "\n",
    "**Proposition:**\n",
    "\n",
    "$$ R^2 = \\frac{Cov(X,Y)^2}{Var(X) \\cdot Var(Y)} = (\\frac{Cov(X,Y)}{\\sigma_X \\cdot \\sigma_Y})^2$$\n",
    "\n",
    "The (Pearson) _correlation_ $\\rho = \\sqrt{R^2}$ is defined as measures how well a linear model fits the plot.\n",
    "\n",
    "By Cauchy-Schwary inequality we have $0\\leq \\rho^2 \\leq 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have covered most important applications of statistics in IT Operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## About the Author\n",
    "\n",
    "Heinrich Hartmann is Chief Data Scientist for Circonus. He earned his\n",
    "PhD in pure Mathematics from the University of Bonn (Germany) on\n",
    "geometric aspects of string theory and worked as a researcher for the\n",
    "University of Oxford (UK) afterwards.  In 2012 he transitioned into\n",
    "computer science and worked as independent consultant for a number of\n",
    "different companies and research institutions. He is now leading the\n",
    "development of data analytics for the Circonus monitoring product.\n",
    "\n",
    "## About Circonus\n",
    "\n",
    "Circonus provides analytics and monitoring for Web-Scale IT. Developed\n",
    "specifically for the requirements of DevOps, the Circonus platform\n",
    "delivers alerts, graphs, dashboards and machine-learning intelligence\n",
    "that help to optimize not just your operations, but also your\n",
    "business. Proprietary Database technology and Analytics tools enable\n",
    "Circonus to provide forensic, predictive, and automated analytics\n",
    "capabilities that no other product can match, and at a scale that\n",
    "other products can only dream of.\n",
    "\n",
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
